{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwHSyAs6cdybetO0wNg/xk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliansgp/Learning-PyTorch/blob/main/CodeMe-com/09_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train simple CNN model for MNIST dataset"
      ],
      "metadata": {
        "id": "yRQ3etHklx8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this website is a playground to work with convolution function on image:\n",
        "\n",
        "https://setosa.io/ev/image-kernels/\n",
        "\n",
        "---\n",
        "\n",
        "kernel/filter in convolution is the matrix which multiply to selected pixel matrix of the image.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "for a CNN , we need conv layers and pooling layers.\n",
        "\n",
        "conv layer will multiply image pixel values to a specific value to make it what we want\n",
        "\n",
        "on other side the pooling layer will reduce image pixels with some methods.for ex. max pooling will group pixel image return max pixel value of every 4 pixel and convert for ex. 8*8 image to 4*4 image.\n",
        "\n",
        "or there is another method named avrage pooling, it will return avarage of each group."
      ],
      "metadata": {
        "id": "Pd3wWlxTlwqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "gMZxfZbilsPx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all we know the data is a 2d gray scaled image\n",
        "# But we want a 4D image (# of images, Height, Width, Color Channels)\n",
        "\n",
        "# Convert MNIST Image Files into a Tensor of 4-Dimensions\n",
        "# this will use when we load out data\n",
        "transform = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "3yJTA30yArDy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load MNIST dataset"
      ],
      "metadata": {
        "id": "ox89ZIeGAlQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data\n",
        "train_data = datasets.MNIST(root='/cnn_data', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtMFiJ-GAkMc",
        "outputId": "05b85569-ea5b-423e-9ae5-bb5f068aac85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /cnn_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 73451946.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /cnn_data/MNIST/raw/train-images-idx3-ubyte.gz to /cnn_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /cnn_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 19443931.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /cnn_data/MNIST/raw/train-labels-idx1-ubyte.gz to /cnn_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /cnn_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28736948.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /cnn_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /cnn_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /cnn_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4252350.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /cnn_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /cnn_data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Data\n",
        "# it will not download again because in train data, the whole data was downloaded\n",
        "test_data = datasets.MNIST(root='/cnn_data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "OJrzZTByA6Zu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bqiSa33BFuF",
        "outputId": "9c754df1-152c-4968-acc1-b5b454c910a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: /cnn_data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-7sHGDqBHg6",
        "outputId": "e9d6e169-d077-4f2a-829e-2926bbab8aaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: /cnn_data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small batch size for images\n",
        "# batch size is usualy small,\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "esWQtCTLBJ8M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Our CNN Model\n",
        "# Describe convolutional layer and what it's doing (2 convolutional layers)\n",
        "# This is just an example in the next video we'll build out the actual model\n",
        "conv1 = nn.Conv2d(1, 6, 3, 1)\n",
        "conv2 = nn.Conv2d(6, 16, 3, 1)\n"
      ],
      "metadata": {
        "id": "Aqw1JFDLM_dY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab 1 MNIST record/image\n",
        "for i, (X_Train, y_train) in enumerate(train_data):\n",
        "  break"
      ],
      "metadata": {
        "id": "3icinYLMM4M0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQrNuCjxM4xn",
        "outputId": "18e15c6f-ea4b-49e3-dda8-86966de656c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = X_Train.view(1,1,28,28)"
      ],
      "metadata": {
        "id": "7KJ2fjJZM6Vz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform our first convolution\n",
        "x = F.relu(conv1(x)) # Rectified Linear Unit for our activation function"
      ],
      "metadata": {
        "id": "nIz-5A_EM7-M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 single image, 6 is the filters we asked for, 26x26\n",
        "x.shape\n",
        "\n",
        "# why 26x26? because arownd the image in this dataset was black and with conv.\n",
        "# it will drop it...\n",
        "# it this case we dont care about it because the important contex is on the middle of image\n",
        "# if its important, pedding in conv2d most be modifed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8micnfLND5p",
        "outputId": "4319984d-e864-4dd0-fd18-d103b2dccda0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass thru the pooling layer\n",
        "x = F.max_pool2d(x,2,2) # kernal of 2 and stride of 2"
      ],
      "metadata": {
        "id": "Ldn4gGz9NF5d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # 26 / 2 = 13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itLQWyD_Ng6U",
        "outputId": "45aa2f77-d2d9-46f8-e72d-400984dc313e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 13, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do our second convolutional layer\n",
        "x = F.relu(conv2(x))"
      ],
      "metadata": {
        "id": "SWxC1eXKNjZj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # Again, we didn't set padding so we lose 2 pixles around the outside of the image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwcVjc89Nlfq",
        "outputId": "cb1e241a-e8dc-4a54-cf40-486746c530c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pooling layer\n",
        "x = F.max_pool2d(x,2,2)"
      ],
      "metadata": {
        "id": "bRDbM3beN-Bf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape # 11 / 2 = 5.5 but we have to round down, because you can't invent data to round up"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSLG0t86OBZl",
        "outputId": "b386e21b-8da4-4533-cbe2-ba44c523132f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((28-2) / 2 - 2) / 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7vvpLbPOGw4",
        "outputId": "bdd42a9c-3aca-470e-ea9d-573a21603a1a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.5"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Class\n",
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1,6,3,1)\n",
        "    self.conv2 = nn.Conv2d(6,16,3,1)\n",
        "    # Fully Connected Layer\n",
        "    self.fc1 = nn.Linear(5*5*16, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.conv1(X))\n",
        "    X = F.max_pool2d(X,2,2) # 2x2 kernal and stride 2\n",
        "    # Second Pass\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.max_pool2d(X,2,2) # 2x2 kernal and stride 2\n",
        "\n",
        "    # Re-View to flatten it out\n",
        "    X = X.view(-1, 16*5*5) # negative one so that we can vary the batch size\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "    return F.log_softmax(X, dim=1)"
      ],
      "metadata": {
        "id": "4WzGjHNvOJFB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Instance of our Model\n",
        "torch.manual_seed(41)\n",
        "model = ConvolutionalNetwork()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFn51LOJOLDn",
        "outputId": "685f885b-6635-4517-979b-f1d634529978"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvolutionalNetwork(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Smaller the Learning Rate, longer its gonna take to train."
      ],
      "metadata": {
        "id": "liS4IuWGPs0m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# monitor time it takes...\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Create Variables To Tracks Things\n",
        "# will be usefull for monitor and plot\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "# For Loop of Epochs\n",
        "for i in range(epochs):\n",
        "  trn_corr = 0\n",
        "  tst_corr = 0\n",
        "\n",
        "\n",
        "  # Train\n",
        "  for b,(X_train, y_train) in enumerate(train_loader):\n",
        "    b+=1 # start our batches at 1\n",
        "    y_pred = model(X_train) # get predicted values from the training set. Not flattened 2D\n",
        "    loss = criterion(y_pred, y_train) # how off are we? Compare the predictions to correct answers in y_train\n",
        "\n",
        "    predicted = torch.max(y_pred.data, 1)[1] # add up the number of correct predictions. Indexed off the first point\n",
        "    batch_corr = (predicted == y_train).sum() # how many we got correct from this batch. True = 1, False=0, sum those up\n",
        "    trn_corr += batch_corr # keep track as we go along in training.\n",
        "\n",
        "    # Update our parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # Print out some results\n",
        "    if b%600 == 0:\n",
        "      print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "\n",
        "  # Test\n",
        "  with torch.no_grad(): #No gradient so we don't update our weights and biases with test data\n",
        "    for b,(X_test, y_test) in enumerate(test_loader):\n",
        "      y_val = model(X_test)\n",
        "      predicted = torch.max(y_val.data, 1)[1] # Adding up correct predictions\n",
        "      tst_corr += (predicted == y_test).sum() # T=1 F=0 and sum away\n",
        "\n",
        "\n",
        "  loss = criterion(y_val, y_test)\n",
        "  test_losses.append(loss)\n",
        "  test_correct.append(tst_corr)\n",
        "\n",
        "\n",
        "\n",
        "current_time = time.time()\n",
        "total = current_time - start_time\n",
        "print(f'Training Took: {total/60} minutes!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOck4DsUPzQE",
        "outputId": "06429a1c-cbe2-46eb-d869-a3f6ccab917b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0  Batch: 600  Loss: 0.1623612344264984\n",
            "Epoch: 0  Batch: 1200  Loss: 0.16147248446941376\n",
            "Epoch: 0  Batch: 1800  Loss: 0.4663747251033783\n",
            "Epoch: 0  Batch: 2400  Loss: 0.1791624128818512\n",
            "Epoch: 0  Batch: 3000  Loss: 0.007612378802150488\n",
            "Epoch: 0  Batch: 3600  Loss: 0.364602267742157\n",
            "Epoch: 0  Batch: 4200  Loss: 0.0038337160367518663\n",
            "Epoch: 0  Batch: 4800  Loss: 0.0014597566332668066\n",
            "Epoch: 0  Batch: 5400  Loss: 0.05793023109436035\n",
            "Epoch: 0  Batch: 6000  Loss: 0.0003491793177090585\n",
            "Epoch: 1  Batch: 600  Loss: 0.0035111396573483944\n",
            "Epoch: 1  Batch: 1200  Loss: 0.2524133622646332\n",
            "Epoch: 1  Batch: 1800  Loss: 0.003167251590639353\n",
            "Epoch: 1  Batch: 2400  Loss: 0.00279847695492208\n",
            "Epoch: 1  Batch: 3000  Loss: 0.019998520612716675\n",
            "Epoch: 1  Batch: 3600  Loss: 0.5321015119552612\n",
            "Epoch: 1  Batch: 4200  Loss: 0.037889160215854645\n",
            "Epoch: 1  Batch: 4800  Loss: 0.00040886964416131377\n",
            "Epoch: 1  Batch: 5400  Loss: 0.0005964030278846622\n",
            "Epoch: 1  Batch: 6000  Loss: 0.3610090911388397\n",
            "Epoch: 2  Batch: 600  Loss: 0.04091913253068924\n",
            "Epoch: 2  Batch: 1200  Loss: 0.0016894402215257287\n",
            "Epoch: 2  Batch: 1800  Loss: 0.0015820137923583388\n",
            "Epoch: 2  Batch: 2400  Loss: 0.005772534757852554\n",
            "Epoch: 2  Batch: 3000  Loss: 0.005380532238632441\n",
            "Epoch: 2  Batch: 3600  Loss: 0.002677877899259329\n",
            "Epoch: 2  Batch: 4200  Loss: 0.019075797870755196\n",
            "Epoch: 2  Batch: 4800  Loss: 0.0012597038876265287\n",
            "Epoch: 2  Batch: 5400  Loss: 0.020566126331686974\n",
            "Epoch: 2  Batch: 6000  Loss: 0.13172155618667603\n",
            "Epoch: 3  Batch: 600  Loss: 0.008950762450695038\n",
            "Epoch: 3  Batch: 1200  Loss: 0.055632926523685455\n",
            "Epoch: 3  Batch: 1800  Loss: 0.002924808068200946\n",
            "Epoch: 3  Batch: 2400  Loss: 5.3784278861712664e-05\n",
            "Epoch: 3  Batch: 3000  Loss: 0.0014280466130003333\n",
            "Epoch: 3  Batch: 3600  Loss: 0.0004992283647879958\n",
            "Epoch: 3  Batch: 4200  Loss: 0.0057424018159508705\n",
            "Epoch: 3  Batch: 4800  Loss: 3.429397474974394e-05\n",
            "Epoch: 3  Batch: 5400  Loss: 0.20988111197948456\n",
            "Epoch: 3  Batch: 6000  Loss: 0.0046822684817016125\n",
            "Epoch: 4  Batch: 600  Loss: 0.01998347043991089\n",
            "Epoch: 4  Batch: 1200  Loss: 0.0015842147404327989\n",
            "Epoch: 4  Batch: 1800  Loss: 0.08000907301902771\n",
            "Epoch: 4  Batch: 2400  Loss: 0.00047235406236723065\n",
            "Epoch: 4  Batch: 3000  Loss: 0.0003118479799013585\n",
            "Epoch: 4  Batch: 3600  Loss: 0.00606974121183157\n",
            "Epoch: 4  Batch: 4200  Loss: 0.00019989915017504245\n",
            "Epoch: 4  Batch: 4800  Loss: 0.013890737667679787\n",
            "Epoch: 4  Batch: 5400  Loss: 0.072105772793293\n",
            "Epoch: 4  Batch: 6000  Loss: 0.02102249301970005\n",
            "Training Took: 2.314700448513031 minutes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8rtdNemRdJ5"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}